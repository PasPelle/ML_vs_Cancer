{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "\n",
    "    #############################\n",
    "    #  1 - Model architecture   #\n",
    "    #############################\n",
    "\n",
    "    # $CHALLENGIFY_BEGIN\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(5, input_dim=5, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='softmax'))\n",
    "    # $CHALLENGIFY_END\n",
    "\n",
    "    #############################\n",
    "    #  2 - Optimization Method  #\n",
    "    #############################\n",
    "    model.compile(loss='categorical_crossentropy', # We've already mentioned this loss function in Logistic Regression\n",
    "                  optimizer='adam', # Optimizer in Deep Learning = solver in Machine Learning | Adam = our best friend\n",
    "                  metrics=['accuracy']) # Let's focus on the accuracy, our dataset is balanced\n",
    "\n",
    "    return model\n",
    "\n",
    "model = initialize_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 5)                 30        \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 36 (144.00 Byte)\n",
      "Trainable params: 36 (144.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = np.linspace(0,1,50)\n",
    "f2 = np.linspace(1,2,50)\n",
    "f3 = np.linspace(0,2,50)\n",
    "f4 = np.linspace(1,2,50)\n",
    "f5 = np.linspace(0,0.5,50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 features\n",
    "\n",
    "X = pd.DataFrame({\n",
    "    \"f1\": f1,\n",
    "    \"f2\": f2,\n",
    "    \"f3\": f3,\n",
    "    \"f4\": f4,\n",
    "    \"f5\": f5\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50,), (50, 5))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.linspace(0,10,50)\n",
    "y.shape, X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40, 5), (40,))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kristjan/.pyenv/versions/3.10.6/envs/ML_vs_Cancer/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1260: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(8, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs = 10,\n",
    "                    batch_size = 8,\n",
    "                    verbose = 0) # Try different verbose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'validation_data': None,\n",
       " 'model': <keras.src.engine.sequential.Sequential at 0x7f7cc4eba290>,\n",
       " '_chief_worker_only': None,\n",
       " '_supports_tf_logs': False,\n",
       " 'history': {'loss': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       "  'accuracy': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]},\n",
       " 'params': {'verbose': 0, 'epochs': 10, 'steps': 5},\n",
       " 'epoch': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.__dict__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
       " 'accuracy': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.title('Train loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 138ms/step - loss: 0.0000e+00 - accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kristjan/.pyenv/versions/3.10.6/envs/ML_vs_Cancer/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py:1260: SyntaxWarning: In loss categorical_crossentropy, expected y_pred.shape to be (batch_size, num_classes) with num_classes > 1. Received: y_pred.shape=(None, 1). Consider using 'binary_crossentropy' if you only have 2 classes.\n",
      "  return dispatch_target(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "res = model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(X_test.iloc[0]).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.897959</td>\n",
       "      <td>1.897959</td>\n",
       "      <td>1.795918</td>\n",
       "      <td>1.897959</td>\n",
       "      <td>0.44898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          f1        f2        f3        f4       f5\n",
       "44  0.897959  1.897959  1.795918  1.897959  0.44898"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kristjan/.pyenv/versions/3.10.6/envs/ML_vs_Cancer/lib/python3.10/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save(\"dummy_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "loaded_model = keras.models.load_model(\"dummy_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.]], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert image to array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "path = \"/home/kristjan/code/ML_vs_Cancer/raw_data/train_subset/a_no_tumor_tissue/0a2ede7cd442222b402ee53050a21b109563b5b5.tif\"\n",
    "image = \n",
    "def read_and_process_image(image):\n",
    "    img = Image.open(file_path)\n",
    "    img = img.resize((224, 224))\n",
    "    return img\n",
    "\n",
    "images = [os.path.join(image_folder_label0, img) if label == 0 else os.path.join(image_folder_label1, img) for img, label in zip(images, labels)]\n",
    "\n",
    "# image_arrays = [read_and_process_image(img) for img in images]\n",
    "image_arrays = [cv2.imread(img_path) for img_path in images]\n",
    "\n",
    "valid_data = [(img, label, img_array) for img, label, img_array in zip(images, labels, image_arrays) if img_array is not None]\n",
    "df = pd.DataFrame(valid_data, columns=['image_path', 'label', 'image_array'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tifffile\n",
      "  Downloading tifffile-2023.9.26-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: numpy in /home/kristjan/.pyenv/versions/3.10.6/envs/ML_vs_Cancer/lib/python3.10/site-packages (from tifffile) (1.26.2)\n",
      "Downloading tifffile-2023.9.26-py3-none-any.whl (222 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.9/222.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tifffile\n",
      "Successfully installed tifffile-2023.9.26\n"
     ]
    }
   ],
   "source": [
    "!pip install tifffile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (96, 96, 3)\n",
      "[[[214 198 224]\n",
      "  [110  94 105]\n",
      "  [188 169 171]\n",
      "  ...\n",
      "  [ 50  18  55]\n",
      "  [ 61  33  73]\n",
      "  [ 59  33  78]]\n",
      "\n",
      " [[255 246 255]\n",
      "  [219 203 216]\n",
      "  [255 244 247]\n",
      "  ...\n",
      "  [ 58  21  55]\n",
      "  [ 28   0  31]\n",
      "  [155 124 165]]\n",
      "\n",
      " [[150 128 164]\n",
      "  [247 228 248]\n",
      "  [225 208 216]\n",
      "  ...\n",
      "  [121  77 110]\n",
      "  [185 141 177]\n",
      "  [255 218 255]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[195 166 188]\n",
      "  [241 208 229]\n",
      "  [193 151 173]\n",
      "  ...\n",
      "  [ 47   4  57]\n",
      "  [129  87 135]\n",
      "  [133  94 139]]\n",
      "\n",
      " [[255 246 255]\n",
      "  [158 125 142]\n",
      "  [225 181 204]\n",
      "  ...\n",
      "  [ 33   1  48]\n",
      "  [ 52  22  60]\n",
      "  [ 97  66 100]]\n",
      "\n",
      " [[115  92 100]\n",
      "  [235 203 218]\n",
      "  [211 167 192]\n",
      "  ...\n",
      "  [ 41  20  61]\n",
      "  [ 55  31  63]\n",
      "  [ 42  18  42]]]\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/home/kristjan/code/ML_vs_Cancer/notebooks/1.tif\"\n",
    "def read_tiff_image(file_path):\n",
    "    try:\n",
    "        # Read the TIFF image using tifffile\n",
    "        tiff_data = tifffile.imread(file_path)\n",
    "\n",
    "        # Convert to NumPy array if not already\n",
    "        if not isinstance(tiff_data, np.ndarray):\n",
    "            tiff_data = np.array(tiff_data)\n",
    "\n",
    "        return tiff_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading TIFF image: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "tif_file_path = file_path\n",
    "image_array = read_tiff_image(file_path)\n",
    "\n",
    "if image_array is not None:\n",
    "    print(\"Image shape:\", image_array.shape)\n",
    "    print(image_array)\n",
    "    # Add your further processing logic here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tiff_image(file):\n",
    "    try:\n",
    "        # Read the TIFF image using tifffile\n",
    "        tiff_data = tifffile.imread(file)\n",
    "\n",
    "        # Convert to NumPy array if not already\n",
    "        if not isinstance(tiff_data, np.ndarray):\n",
    "            tiff_data = np.array(tiff_data)\n",
    "\n",
    "        return tiff_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading TIFF image: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (96, 96, 3)\n",
      "[[[214 198 224]\n",
      "  [110  94 105]\n",
      "  [188 169 171]\n",
      "  ...\n",
      "  [ 50  18  55]\n",
      "  [ 61  33  73]\n",
      "  [ 59  33  78]]\n",
      "\n",
      " [[255 246 255]\n",
      "  [219 203 216]\n",
      "  [255 244 247]\n",
      "  ...\n",
      "  [ 58  21  55]\n",
      "  [ 28   0  31]\n",
      "  [155 124 165]]\n",
      "\n",
      " [[150 128 164]\n",
      "  [247 228 248]\n",
      "  [225 208 216]\n",
      "  ...\n",
      "  [121  77 110]\n",
      "  [185 141 177]\n",
      "  [255 218 255]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[195 166 188]\n",
      "  [241 208 229]\n",
      "  [193 151 173]\n",
      "  ...\n",
      "  [ 47   4  57]\n",
      "  [129  87 135]\n",
      "  [133  94 139]]\n",
      "\n",
      " [[255 246 255]\n",
      "  [158 125 142]\n",
      "  [225 181 204]\n",
      "  ...\n",
      "  [ 33   1  48]\n",
      "  [ 52  22  60]\n",
      "  [ 97  66 100]]\n",
      "\n",
      " [[115  92 100]\n",
      "  [235 203 218]\n",
      "  [211 167 192]\n",
      "  ...\n",
      "  [ 41  20  61]\n",
      "  [ 55  31  63]\n",
      "  [ 42  18  42]]]\n"
     ]
    }
   ],
   "source": [
    "# Open the TIFF file in binary mode and pass the file object to the function\n",
    "with open(file_path, 'rb') as tiff_file:\n",
    "    image_array = read_tiff_image(tiff_file)\n",
    "\n",
    "if image_array is not None:\n",
    "    print(\"Image shape:\", image_array.shape)\n",
    "    print(image_array)\n",
    "    # Add your further processing logic here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
