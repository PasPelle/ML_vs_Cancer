{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_-er5ygaHsd"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "\n",
        "\n",
        "#load the VGG16 model\n",
        "\n",
        "def load_model():\n",
        "\n",
        "    model = VGG16(weights=\"imagenet\", include_top=False, input_shape=X_train[0].shape)\n",
        "\n",
        "\n",
        "    return model\n",
        "\n",
        "model = load_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#deactivating the training of the VGG16 paramters\n",
        "\n",
        "def set_nontrainable_layers(model):\n",
        "\n",
        "    # Set the first layers to be untrainable\n",
        "    model.trainable = False\n",
        "\n",
        "    return model\n",
        "\n",
        "model = set_nontrainable_layers(model)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "KS88q7ohef8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#chaining the pretrained convolutional layers of VGG16 with our own dense layers\n",
        "\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def add_last_layers(model):\n",
        "    '''Take a pre-trained model, set its parameters as non-trainable, and add additional trainable layers on top'''\n",
        "    base_model = set_nontrainable_layers(model)\n",
        "    flatten_layer = layers.Flatten()\n",
        "    dense_layer = layers.Dense(500, activation='relu')\n",
        "    prediction_layer = layers.Dense(3, activation='softmax')\n",
        "\n",
        "\n",
        "    model = models.Sequential([\n",
        "        base_model,\n",
        "        flatten_layer,\n",
        "        dense_layer,\n",
        "        prediction_layer\n",
        "    ])\n",
        "\n",
        "    opt = optimizers.Adam(learning_rate=1e-4)\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=opt,\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "model = build_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "CIcYfcRqe8Fm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "model = build_model()\n",
        "\n",
        "es = EarlyStopping(monitor = 'val_accuracy',\n",
        "                   mode = 'max',\n",
        "                   patience = 5,\n",
        "                   verbose = 1,\n",
        "                   restore_best_weights = True)\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    epochs=50,\n",
        "                    batch_size=16,\n",
        "                    callbacks=[es])"
      ],
      "metadata": {
        "id": "XF_ath-ghkI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(history, title='', axs=None, exp_name=\"\"):\n",
        "    if axs is not None:\n",
        "        ax1, ax2 = axs\n",
        "    else:\n",
        "        f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "    if len(exp_name) > 0 and exp_name[0] != '_':\n",
        "        exp_name = '_' + exp_name\n",
        "    ax1.plot(history.history['loss'], label='train' + exp_name)\n",
        "    ax1.plot(history.history['val_loss'], label='val' + exp_name)\n",
        "    #ax1.set_ylim(0., 2.2)\n",
        "    ax1.set_title('loss')\n",
        "    ax1.legend()\n",
        "\n",
        "    ax2.plot(history.history['accuracy'], label='train accuracy'  + exp_name)\n",
        "    ax2.plot(history.history['val_accuracy'], label='val accuracy'  + exp_name)\n",
        "    #ax2.set_ylim(0.25, 1.)\n",
        "    ax2.set_title('Accuracy')\n",
        "    ax2.legend()\n",
        "    return (ax1, ax2)\n",
        "\n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "F1LR2RJYhoMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Comparing the performances\n",
        "\n",
        "test_accuracy_vgg = res_aug[-1]\n",
        "\n",
        "\n",
        "\n",
        "print(f\"test_accuracy = {round(test_accuracy,2)*100} %\")\n",
        "\n",
        "print(f\"test_accuracy_vgg = {round(test_accuracy_vgg,2)*100} %\")\n"
      ],
      "metadata": {
        "id": "ep08cg2ClPcs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}